{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The-Elite-Sem-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddesai-sjsu/Machine-Learning/blob/main/The_Elite_Sem_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OttBdxs1rb4m"
      },
      "source": [
        "# **Team Member: Deesha Desai**\n",
        "### **Sentiment Factor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D6ZgVGPqpCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cf1ecd-23b7-488a-cfc9-bfb40cf4db8b"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPCcADGlqwwU"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x54qWxy_qz8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1998a3d7-50b5-4210-e51c-eeab0a9cedb6"
      },
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Error loading vader: Package 'vader' not found in index\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJzvQQ_DrJ51"
      },
      "source": [
        "### **Downloading pickled models of TFIDF Vectorizer & Neural Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KdYIs5kq0II",
        "outputId": "017bde1e-4d84-489f-c3fa-af0c6c8e4a95"
      },
      "source": [
        "!gdown --id 1Yuxk8aC7PTQJwJz8mw1Xn6uPyEU_vhm6\n",
        "!gdown --id 1gabfL1LLI-iOtc9f5JI5EUyqQuHPHtku\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Yuxk8aC7PTQJwJz8mw1Xn6uPyEU_vhm6\n",
            "To: /content/Neural Net\n",
            "100% 766k/766k [00:00<00:00, 51.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gabfL1LLI-iOtc9f5JI5EUyqQuHPHtku\n",
            "To: /content/tfidf\n",
            "100% 160k/160k [00:00<00:00, 58.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUx5A2vxLHfu"
      },
      "source": [
        "### **Generating POS Tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfpbnYF8rnxe"
      },
      "source": [
        "def get_POS_tags(statement):\n",
        "  tokens = nltk.word_tokenize(statement)\n",
        "  text = nltk.Text(tokens)\n",
        "  tags = nltk.pos_tag(text)\n",
        "  counts = Counter(tag for word,tag in tags)\n",
        "  noun_counts=counts['NN']+counts['NNS']+counts['NNP']+counts['NNPS']\n",
        "  verb_counts=counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBG']\n",
        "  adverbverb_counts=counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBG']\n",
        "  adjective_counts=counts['JJ']+counts['JJR']+counts['JJS']\n",
        "  possessive_ending_counts=counts['POS']\n",
        "  return noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl_1owXqLgAX"
      },
      "source": [
        "### **Generating sentiment score**\n",
        "\n",
        "Using Neural Net pickled model & tfidf vectorizer to predict sentiment score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Wppp6zq7uo"
      },
      "source": [
        "def get_sentiment_score(statement):\n",
        "  model=pickle.load(open('/content/Neural Net', 'rb'))\n",
        "  tfidf_vect=pickle.load(open('/content/tfidf', 'rb'))\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  lem=WordNetLemmatizer()\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "  p_statement= re.sub(r\"\\W\", \" \",str(statement))\n",
        "  p_statement=re.sub(r'[,\\.!?]', \" \",str(p_statement))\n",
        "  p_statement=re.sub(r\" \\d+\", \" \",str(p_statement))\n",
        "  p_statement=word_tokenize(p_statement.lower())\n",
        "  p_statement=' '.join(p_statement)\n",
        "  p_statement=' '.join([word for word in p_statement.split() if word not in stop_words])\n",
        "  p_statement=' '.join([lem.lemmatize(word) for word in p_statement.split()])\n",
        "  p_statement=' '.join([stemmer.stem(word) for word in p_statement.split()])\n",
        "  noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts=get_POS_tags(p_statement)\n",
        "\n",
        "  compound= analyzer.polarity_scores(p_statement)['compound']\n",
        "  neg=  analyzer.polarity_scores(p_statement)['neg'] \n",
        "  neu = analyzer.polarity_scores(p_statement)['neu']\n",
        "  pos = analyzer.polarity_scores(p_statement)['pos']\n",
        "\n",
        "  X=[noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts,compound,neg,neu,pos]\n",
        "  vector=tfidf_vect.transform([p_statement])\n",
        "\n",
        "  final_input=scipy.sparse.hstack((vector,X),format='csr')\n",
        "  \n",
        "  prediction=model.predict(final_input)\n",
        "  prediction_probability=model.predict_proba(final_input)[:,1]\n",
        "  return prediction_probability.item(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6oyLJTq9r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7051eeb-d4e6-4a43-9301-663a13ec983f"
      },
      "source": [
        "get_sentiment_score('sad')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3001182723057658"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnEn5tcAwF9S"
      },
      "source": [
        "# **Team member - Preeti Parihar**\n",
        "### **Topic - Speech Tone Detection**\n",
        "\n",
        "#### **Factor - Toxicity**\n",
        "#### **Micro Factor**\n",
        "  * Severe Toxic\n",
        "  * Obscene\n",
        "  * Threat\n",
        "  * Insult\n",
        "  * Identity attack\n",
        "  * Hate Speech\n",
        "  * Polarity\n",
        "  * Readability\n",
        "  * Offensive language\n",
        "  * Target\n",
        "\n",
        "### **Datasets**\n",
        "  * Jigsaw Dataset\n",
        "  * Twitter Hate Speech\n",
        "  * Stream Data - https://newsapi.org\n",
        "  * Politi Fact - For Inference\n",
        "\n",
        "### **Models Generated For Following Feature Individually**\n",
        "  * Obscene\n",
        "  * Threat\n",
        "  * Insult\n",
        "  * Identity attack\n",
        "  * Hate Speech\n",
        "  * Polarity\n",
        "  * Readability\n",
        "  * Target\n",
        "\n",
        "### **List of ML operations**\n",
        "  * **Get Top K Predictions**\n",
        "  * **Generated Models Using Muller Loop**\n",
        "    Following list of classifiers were used to train the model and selected best model.\n",
        "    * Logistic Regression\n",
        "    * Stochastic Gradient Descent (SGD) Classifier\n",
        "    * XGB Classifier\n",
        "    * Random Forest Classifier\n",
        "  * **Load model and transformer**\n",
        "  Load model and transformer from google drive\n",
        "  * **Save Model**\n",
        "  Save model and transformer to google drive\n",
        "  * **Infer and Load**\n",
        "  Run inference on unseen data after loading model and transformer\n",
        "\n",
        "### **Combined All ML Models**\n",
        "  * Using polynomial equations (weighted model scores)\n",
        "\n",
        "### **Results**\n",
        "  * Result printed using Truth-O-Meter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c-BH_Kd98F5"
      },
      "source": [
        "The ARI assesses the U.S. grade level required to read a piece of text.\n",
        "\n",
        "https://readable.com/readability/automated-readability-index/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_vVDI-pv9gT"
      },
      "source": [
        "def readability_grade(score):\n",
        "    if score <= 6:\n",
        "        return \"Kindergarten\"\n",
        "    elif score <= 7:\n",
        "        return \"First Grade\"\n",
        "    elif score <= 8:\n",
        "        return \"Second Grade\"\n",
        "    elif score <= 9:\n",
        "        return \"Third Grade\"\n",
        "    elif score <= 10:\n",
        "        return \"Fourth Grade\"\n",
        "    elif score <= 11:\n",
        "        return \"Fifth Grade\"\n",
        "    elif score <= 12:\n",
        "        return \"Sixth Grade\"\n",
        "    elif score <= 13:\n",
        "        return \"Seventh Grade\"\n",
        "    elif score <= 14:\n",
        "        return \"Eight Grade\"\n",
        "    elif score <= 15:\n",
        "        return \"Ninth Grade\"\n",
        "    elif score <= 15:\n",
        "        return \"Ninth Grade\"\n",
        "    elif score <= 16:\n",
        "        return \"Tenth Grade\"\n",
        "    elif score <= 17:\n",
        "        return \"Eleventh Grade\"\n",
        "    elif score <= 17:\n",
        "        return \"Twelfth Grade\"\n",
        "    elif score >= 18:\n",
        "        return \"College Grade\"\n",
        "\n",
        "def get_top_k_predictions(model, X_test, k):\n",
        "    import numpy as np\n",
        "    # get probabilities instead of predicted labels, since we want to collect top 3\n",
        "    probs = model.predict_proba(X_test)\n",
        "    top_k_probs = probs[0][:k]\n",
        "    # print(top_k_probs)\n",
        "    # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
        "    best_n = np.argsort(probs, axis=1)[:,-k:]\n",
        "    # print(best_n)\n",
        "    \n",
        "    # GET CATEGORY OF PREDICTIONS\n",
        "    preds=[[(model.classes_[predicted_cat], probs) for predicted_cat, probs in zip(prediction, top_k_probs)] for prediction in best_n]\n",
        "    \n",
        "    preds=[ item[::-1] for item in preds]\n",
        "    \n",
        "    return preds\n",
        "\n",
        "\n",
        "def load_model_transformer(model_name, transformer_name):\n",
        "  import pickle\n",
        "  try:\n",
        "    model_path = \"/content/drive/MyDrive/The-Elite/Semester_Project/Preeti/models/\" + model_name\n",
        "    model_file = open(model_path, \"rb\")\n",
        "    model = pickle.load(model_file)\n",
        "\n",
        "    transformer_path = \"/content/drive/MyDrive/The-Elite/Semester_Project/Preeti/models/\" + transformer_name\n",
        "    transformer_file = open(transformer_path, \"rb\")\n",
        "    transformer = pickle.load(transformer_file)\n",
        "    return model, transformer, None\n",
        "\n",
        "  except Exception as e:\n",
        "    return None, None, e\n",
        "\n",
        "def load_and_infer(model_name, transformer_name, text, feature, top_k=2):\n",
        "  model, transformer, err = load_model_transformer(model_name, transformer_name)\n",
        "  if err:\n",
        "    print(str(err))\n",
        "    return\n",
        "\n",
        "  X_test = transformer.transform(text)\n",
        "  preds = get_top_k_predictions(model, X_test, top_k)\n",
        "\n",
        "  if feature.lower() == \"readability\":\n",
        "    grades = []\n",
        "    for c in preds[0]:\n",
        "      grades.append((readability_grade(c[0]), c[1]))\n",
        "\n",
        "    grades = sorted(grades, key=lambda x: x[1], reverse=True)      \n",
        "    return grades\n",
        "  elif feature.lower() == \"polarity\":\n",
        "    sentiments = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        sentiments.append((\"neutral\", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        sentiments.append((\"positive\", c[1]))\n",
        "      elif c[0] == -1:\n",
        "        sentiments.append((\"negative\", c[1]))\n",
        "    sentiments = sorted(sentiments, key=lambda x: x[1], reverse=True)\n",
        "    return sentiments  \n",
        "  elif feature.lower() == \"hate-speech\":\n",
        "    hate_speech = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        hate_speech.append((\"neither\", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        hate_speech.append((\"offensive language\", c[1]))\n",
        "      elif c[0] == 2:\n",
        "        hate_speech.append((\"hate speech\", c[1]))\n",
        "    hate_speech = sorted(hate_speech, key=lambda x: x[1], reverse=True)        \n",
        "    return hate_speech  \n",
        "  elif feature.lower() == \"toxicity\":\n",
        "    toxicity = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        toxicity.append((\"not toxic \", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        toxicity.append((\"toxic\", c[1]))\n",
        "    toxicity = sorted(toxicity, key=lambda x: x[1], reverse=True)        \n",
        "    return toxicity  \n",
        "  elif feature.lower() == \"threat\":\n",
        "    threat = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        threat.append((\"no threat \", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        threat.append((\"threat\", c[1]))\n",
        "    threat = sorted(threat, key=lambda x: x[1], reverse=True)        \n",
        "    return threat  \n",
        "  elif feature.lower() == \"insult\":\n",
        "    insult = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        insult.append((\"no insult \", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        insult.append((\"insult\", c[1]))\n",
        "    insult = sorted(insult, key=lambda x: x[1], reverse=True)        \n",
        "    return insult  \n",
        "  elif feature.lower() == \"obscene\":\n",
        "    obscene = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        obscene.append((\"not obscene \", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        obscene.append((\"obscene\", c[1]))\n",
        "    obscene = sorted(obscene, key=lambda x: x[1], reverse=True)        \n",
        "    return obscene  \n",
        "  elif feature.lower() == \"identity_attack\":\n",
        "    identity_attack = []\n",
        "    for c in preds[0]:\n",
        "      if c[0] == 0:\n",
        "        identity_attack.append((\"no identity attack \", c[1]))\n",
        "      elif c[0] == 1:\n",
        "        identity_attack.append((\"identity attack\", c[1]))\n",
        "    identity_attack = sorted(identity_attack, key=lambda x: x[1], reverse=True)        \n",
        "    return identity_attack  \n",
        "\n",
        "  else:\n",
        "    return \"invalid feature name\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrUlziej-oUE"
      },
      "source": [
        "## **Combine All Models Using Polynomial Equation (weighted model score)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQgPUlGAxVHl"
      },
      "source": [
        "def combined_models(text):\n",
        "  all_acc_score = [0.9975, 0.9875, 0.985, 0.8925, 0.8680, 0.8412, 0.1315]\n",
        "  all_acc_score.sort(reverse=True)\n",
        "  weights = [float(i)/sum(all_acc_score) for i in all_acc_score]\n",
        "\n",
        "  prob_toxicity = weights[0] * load_and_infer(\"model_toxicity.model\", \"toxicity_transformer.pkl\", [text], \"toxicity\", 1)[0][1]\n",
        "  prob_threat = weights[1] * load_and_infer(\"model_threat.model\", \"threat_transformer.pkl\", [text], \"threat\", 1)[0][1]\n",
        "  prob_obscene = weights[2] * load_and_infer(\"model_obscene.model\", \"obscene_transformer.pkl\", [text], \"obscene\", 1)[0][1]\n",
        "  prob_identity_attack = weights[3] * load_and_infer(\"model_identity_attack.model\", \"identity_attack_transformer.pkl\", [text], \"identity_attack\", 1)[0][1]\n",
        "  prob_readability = weights[4] * load_and_infer(\"model_readability.model\", \"readability_transformer.pkl\", [text], \"readability\", 1)[0][1]\n",
        "  prob_polarity = weights[5] *  load_and_infer(\"model_polarity.model\", \"polarity_transformer.pkl\", [text], \"polarity\", 1)[0][1]\n",
        "  prob_hate_speech = weights[6] * load_and_infer(\"model_hate_speech.model\", \"hate_speech_transformer.pkl\", [text], \"hate-speech\", 1)[0][1]\n",
        "\n",
        "  sumW = prob_toxicity + prob_threat + prob_obscene + prob_identity_attack + prob_readability + prob_polarity + prob_hate_speech\n",
        "  if sumW <= 0.20:\n",
        "    return \"Polite: \" + str(round(sumW, 2)),sumW \n",
        "  elif sumW > 0.20 and sumW <= 0.50:\n",
        "    return \"Insult: \" + str(round(sumW, 2)),sumW\n",
        "  elif sumW > 0.50 and sumW <= 0.65:\n",
        "    return \"Toxic: \" + str(round(sumW, 2)),sumW\n",
        "  elif sumW > 0.65 and sumW <= 0.70:\n",
        "    return \"Severe Toxic: \" + str(round(sumW, 2)),sumW\n",
        "  elif sumW > 0.70 and sumW <= 0.85:\n",
        "    return \"Hate Speech: \" + str(round(sumW, 2)),sumW\n",
        "  elif sumW > 0.85:\n",
        "    return \"Threat: \" + str(round(sumW, 2)),sumW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsBdvqZUxljo",
        "outputId": "b6552f24-bbad-4966-f0f7-31f8fcc3028c"
      },
      "source": [
        "result = combined_models(\"Don't dare to talk to me like that, I will kill you\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7048157039283495\n",
            "Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwk1Vm1p-fBW"
      },
      "source": [
        "## **Run Inference on all combined ML models from Politi Fact scrapped data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsCeM1Z_xpEw",
        "outputId": "78e59888-c66d-44e8-871e-54b1cb7b7220"
      },
      "source": [
        "import pandas as pd\n",
        "liar_df = pd.read_csv(\"/content/drive/MyDrive/The-Elite/Semester_Project/Preeti/datasets/politifact.csv\")\n",
        "\n",
        "\n",
        "for id, stmt in enumerate(liar_df.statement):\n",
        "  out = combined_models(stmt)\n",
        "  print(out)\n",
        "  if id > 20:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hate Speech: 0.73\n",
            "Severe Toxic: 0.68\n",
            "Hate Speech: 0.71\n",
            "Hate Speech: 0.71\n",
            "Hate Speech: 0.72\n",
            "Severe Toxic: 0.7\n",
            "Hate Speech: 0.71\n",
            "Severe Toxic: 0.69\n",
            "Severe Toxic: 0.69\n",
            "Hate Speech: 0.72\n",
            "Hate Speech: 0.71\n",
            "Hate Speech: 0.7\n",
            "Severe Toxic: 0.67\n",
            "Severe Toxic: 0.7\n",
            "Hate Speech: 0.71\n",
            "Hate Speech: 0.72\n",
            "Severe Toxic: 0.69\n",
            "Severe Toxic: 0.69\n",
            "Hate Speech: 0.72\n",
            "Severe Toxic: 0.65\n",
            "Severe Toxic: 0.68\n",
            "Severe Toxic: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXKbS-ej-JCa"
      },
      "source": [
        "### **Read live NEWS articles**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlVHvPyY30my"
      },
      "source": [
        "ARTICLE_COUNT = 20\n",
        "def get_news_articles():\n",
        "    API_KEY = \"f682b24e753f472ea186141ff4f68328\"\n",
        "    url = \"https://newsapi.org/v2/top-headlines?country=us&category=business&apiKey=\" + API_KEY\n",
        "    resp = requests.get(url)\n",
        "    if resp and resp.status_code == 200:\n",
        "        \n",
        "        return resp.json()[\"articles\"][:ARTICLE_COUNT]\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0K8zaQV6zn5",
        "outputId": "e581fefe-645e-4ccf-8037-a8abc1b6c1da"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsdJlsT_-QqP"
      },
      "source": [
        "### **Convert HTML to text and Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaBwYQVN6bfq"
      },
      "source": [
        "def html_to_text(url):\n",
        "  from bs4 import BeautifulSoup\n",
        "  from bs4.element import Comment\n",
        "  import urllib.request\n",
        "\n",
        "  def tag_visible(element):\n",
        "      if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
        "          return False\n",
        "      if isinstance(element, Comment):\n",
        "          return False\n",
        "      return True\n",
        "\n",
        "\n",
        "  def text_from_html(body):\n",
        "      soup = BeautifulSoup(body, 'html.parser')\n",
        "      texts = soup.findAll(text=True)\n",
        "      visible_texts = filter(tag_visible, texts)  \n",
        "      return u\" \".join(t.strip() for t in visible_texts)\n",
        "\n",
        "\n",
        "  html = urllib.request.urlopen(url).read()\n",
        "  page_text = text_from_html(html)\n",
        "  my_list = nltk.tokenize.sent_tokenize(page_text) #Split into list of sentences.\n",
        "  content = \"\\n\".join(my_list)\n",
        "  return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLCO2-Fk-SKS"
      },
      "source": [
        "## **Run Inference on all combined ML models from live streamed data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myzxT0kv6elB"
      },
      "source": [
        "import requests\n",
        "def InferOnStreamingData():\n",
        "  news_articles = get_news_articles()\n",
        "  for id, article in enumerate(news_articles):\n",
        "      try:\n",
        "          url = article[\"url\"]\n",
        "          print(\"Reading article:\", str(id), \" at URL: \", url)\n",
        "          content = html_to_text(url)\n",
        "          result = combined_models(content)                    \n",
        "          print(\"Result: \", result)\n",
        "      except Exception as e:\n",
        "          print(\"Error while fetching: \", url, \", error:\", str(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwB-9WTW6gwk",
        "outputId": "1094283b-413e-43d1-eff5-19d07f73749b"
      },
      "source": [
        "InferOnStreamingData()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading article: 0  at URL:  https://www.ft.com/content/c30cf911-51da-4b40-a969-161351de6f04\n",
            "Result:  Hate Speech: 0.71\n",
            "Reading article: 1  at URL:  https://www.investors.com/market-trend/stock-market-today/dow-jones-futures-market-rallies-be-wary-tesla-rivals-lucid-rivian-xpeng-li-auto-skid-docusign-jobs-report/\n",
            "Error while fetching:  https://www.investors.com/market-trend/stock-market-today/dow-jones-futures-market-rallies-be-wary-tesla-rivals-lucid-rivian-xpeng-li-auto-skid-docusign-jobs-report/ , error: HTTP Error 403: Forbidden\n",
            "Reading article: 2  at URL:  https://www.cnn.com/2021/12/02/investing/grab-ipo-spac-nasdaq-intl-hnk/index.html\n",
            "Result:  Severe Toxic: 0.7\n",
            "Reading article: 3  at URL:  https://www.ketv.com/article/tentative-deal-reached-between-kelloggs-and-union-workers/38413959\n",
            "Result:  Severe Toxic: 0.67\n",
            "Reading article: 4  at URL:  https://www.foxbusiness.com/politics/romney-ray-dalio-china-investments-sad-moral-lapse\n",
            "Result:  Hate Speech: 0.7\n",
            "Reading article: 5  at URL:  https://nypost.com/2021/12/02/alabama-fedex-driver-accused-of-dumping-hundreds-of-packages/\n",
            "Result:  Severe Toxic: 0.68\n",
            "Reading article: 6  at URL:  https://cryptobriefing.com/india-may-not-ban-crypto-says-ex-finance-secretary/\n",
            "Error while fetching:  https://cryptobriefing.com/india-may-not-ban-crypto-says-ex-finance-secretary/ , error: HTTP Error 403: Forbidden\n",
            "Reading article: 7  at URL:  https://news.bitcoin.com/meta-relaxes-facebooks-cryptocurrency-ad-policy-crypto-mature-stabilize/\n",
            "Error while fetching:  https://news.bitcoin.com/meta-relaxes-facebooks-cryptocurrency-ad-policy-crypto-mature-stabilize/ , error: HTTP Error 403: Forbidden\n",
            "Reading article: 8  at URL:  https://www.barrons.com/articles/november-jobs-report-2021-51638492490\n",
            "Error while fetching:  https://www.barrons.com/articles/november-jobs-report-2021-51638492490 , error: HTTP Error 403: Forbidden\n",
            "Reading article: 9  at URL:  https://www.foxnews.com/media/philip-bump-dccc-gas-prices-biden\n",
            "Result:  Hate Speech: 0.71\n",
            "Reading article: 10  at URL:  https://www.anandtech.com/show/17101/united-states-ftc-files-lawsuit-to-block-nvidiaarm-acquisition\n",
            "Result:  Hate Speech: 0.71\n",
            "Reading article: 11  at URL:  https://www.cnbc.com/2021/12/02/cramers-lightning-round-vir-and-gsk-may-have-something-that-could-fight-omicron.html\n",
            "Result:  Severe Toxic: 0.68\n",
            "Reading article: 12  at URL:  https://www.youtube.com/watch?v=0ErMyvjowfc\n",
            "Result:  Severe Toxic: 0.69\n",
            "Reading article: 13  at URL:  https://www.theverge.com/2021/12/2/22814879/google-return-to-office-covid-omicron-hybrid-work-plans-2022\n",
            "Result:  Severe Toxic: 0.67\n",
            "Reading article: 14  at URL:  https://www.nytimes.com/2021/12/02/business/yellen-inflation-omicron.html\n",
            "Result:  Severe Toxic: 0.68\n",
            "Reading article: 15  at URL:  https://www.cnn.com/videos/business/2021/12/02/climate-aviation-fuel-lead-muntean-pkg-vpx.cnn\n",
            "Result:  Hate Speech: 0.7\n",
            "Reading article: 16  at URL:  https://www.nytimes.com/2021/12/02/business/media/buzzfeed-spac.html\n",
            "Result:  Severe Toxic: 0.67\n",
            "Reading article: 17  at URL:  https://www.foxbusiness.com/lifestyle/cbp-seizes-30m-worth-fake-gucci-chanel-louis-vuitton-products-china\n",
            "Result:  Hate Speech: 0.71\n",
            "Reading article: 18  at URL:  https://www.newsweek.com/man-who-put-razors-screws-pizza-dough-sentenced-nearly-5-years-prison-1655581\n",
            "Error while fetching:  https://www.newsweek.com/man-who-put-razors-screws-pizza-dough-sentenced-nearly-5-years-prison-1655581 , error: HTTP Error 403: Forbidden\n",
            "Reading article: 19  at URL:  https://www.usatoday.com/story/money/shopping/2021/12/02/bath-body-works-candle-day-sale/8840861002/\n",
            "Result:  Hate Speech: 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUjQHDZ76iuc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dp_5TcHMXtX"
      },
      "source": [
        "# **Team Member-Priyanka Devendran**\n",
        "\n",
        "## **Stance Factor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsksLmgLIvpM",
        "outputId": "948d2d0f-6884-4bd8-eff1-1a4737461ccd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import joblib \n",
        "import re\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "import gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsSgmk4FIv-T",
        "outputId": "195e4278-e44c-437a-d166-5828211dd83c"
      },
      "source": [
        "url='https://drive.google.com/uc?id=19_OSXUTqzkkmZfHbB84B1zJwNjN2Fdxe'\n",
        "output= \"le_speaker_job_title_code.pkl\"\n",
        "gdown.download(url,output,quiet=False)\n",
        "le_speaker_job_title_code=joblib.load('le_speaker_job_title_code.pkl')\n",
        "\n",
        "url='https://drive.google.com/uc?id=1MsEv8mclaVusSe9C7h_2jlCXusSM1dwB'\n",
        "output= \"le_speaker.pkl\"\n",
        "gdown.download(url,output,quiet=False)\n",
        "le_speaker=joblib.load('le_speaker.pkl')\n",
        "\n",
        "url='https://drive.google.com/uc?id=1TBALWJjMRSBPCrS0D1m49784aE3F2Bb4'\n",
        "output= \"RForest.pkl\"\n",
        "gdown.download(url,output,quiet=False)\n",
        "loaded_model=joblib.load('RForest.pkl')\n",
        "\n",
        "url='https://drive.google.com/uc?id=1QIYM0h6v6zc84y-rvCGbi_aGwJhmlHSL'\n",
        "output= \"le_state_info_code.pkl\"\n",
        "gdown.download(url,output,quiet=False)\n",
        "le_state_info_code=joblib.load('le_state_info_code.pkl')\n",
        "\n",
        "url='https://drive.google.com/uc?id=1FVqiACCZHDr-4NDYYT8RWUxz6Ch28-TC'\n",
        "output= \"vectorizer.pkl\"\n",
        "gdown.download(url,output,quiet=False)\n",
        "vector=joblib.load('vectorizer.pkl')\n",
        "\n",
        "url='https://drive.google.com/uc?id=1zV2giAiQI6Qquzf1KZhGJ2o1Z1qee1ad'\n",
        "output= \"XGB.pkl\"\n",
        "gdown.download(url,output,quiet=False)\n",
        "model=joblib.load('XGB.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19_OSXUTqzkkmZfHbB84B1zJwNjN2Fdxe\n",
            "To: /content/le_speaker_job_title_code.pkl\n",
            "100%|██████████| 46.7k/46.7k [00:00<00:00, 26.9MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MsEv8mclaVusSe9C7h_2jlCXusSM1dwB\n",
            "To: /content/le_speaker.pkl\n",
            "100%|██████████| 72.7k/72.7k [00:00<00:00, 66.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TBALWJjMRSBPCrS0D1m49784aE3F2Bb4\n",
            "To: /content/RForest.pkl\n",
            "100%|██████████| 43.8M/43.8M [00:00<00:00, 157MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QIYM0h6v6zc84y-rvCGbi_aGwJhmlHSL\n",
            "To: /content/le_state_info_code.pkl\n",
            "100%|██████████| 1.94k/1.94k [00:00<00:00, 1.98MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FVqiACCZHDr-4NDYYT8RWUxz6Ch28-TC\n",
            "To: /content/vectorizer.pkl\n",
            "100%|██████████| 2.20M/2.20M [00:00<00:00, 98.8MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zV2giAiQI6Qquzf1KZhGJ2o1Z1qee1ad\n",
            "To: /content/XGB.pkl\n",
            "100%|██████████| 341k/341k [00:00<00:00, 46.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sTxVKcpI3gc",
        "outputId": "8e77cd48-46f5-4f82-d3c3-c7d1c17d5d9c"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "vector=joblib.load('vectorizer.pkl')\n",
        "loaded_model=joblib.load('RForest.pkl')\n",
        "def get_stance(statement):\n",
        "  new_statement= re.sub(r\"\\W\", \" \",str(statement))\n",
        "  new_statement=re.sub(r'[,\\.!?]', \" \",str(new_statement))\n",
        "  new_statement=re.sub(r\" \\d+\", \" \",str(new_statement))\n",
        "  new_statement=word_tokenize(new_statement.lower())\n",
        "  new_statement=' '.join(new_statement)\n",
        "  new_statement=' '.join([word for word in new_statement.split() if word not in stop_words])\n",
        "  liar_model = vector.transform([new_statement,])\n",
        "  stance=loaded_model.predict(liar_model)\n",
        "  return stance[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.22.2.post1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_6pWwjlI6ez"
      },
      "source": [
        "def getStanceScore(statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,speaker,state_info,speaker_job_title):\n",
        "  speaker_job_title=le_speaker_job_title_code.transform([speaker_job_title])\n",
        "  speaker=le_speaker.transform([speaker])\n",
        "  state_info=le_state_info_code.transform([state_info])\n",
        "  df=pd.DataFrame([barely_true_counts])\n",
        "  df.columns=['barely_true_counts']\n",
        "  df['false_counts']=false_counts\n",
        "  df['half_true_counts']=half_true_counts\n",
        "  df['mostly_true_counts']=mostly_true_counts\n",
        "  df['pants_on_fire_counts']=pants_on_fire_counts\n",
        "  df['Stance']=get_stance(statement)\n",
        "  df['speaker_code']=speaker\n",
        "  df['state info_code']=state_info\n",
        "  df['speaker_job_title_code']=speaker_job_title\n",
        " \n",
        "  # X=np.array(barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,Stance,speaker,speaker_job_title,state_info)\n",
        "  # ['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'Stance', 'speaker_code', 'state info_code', 'speaker_job_title_code'] \n",
        "  # ['barely_true_counts', 'pants_on_fire_counts', 'speaker_code', 'mostly_true_counts', 'Stance', 'half_true_counts', 'false_counts', 'state info_code', 'speaker_job_title_code']\n",
        "  prediction=model.predict(df)\n",
        "  prediction_probability=model.predict_proba(df)\n",
        "  return prediction_probability.tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiPSA5N8MxPu"
      },
      "source": [
        "#**Combining All Factors**\n",
        "##**Generating Truthfulness on True-O-Meter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T807QC54I8xY"
      },
      "source": [
        " def isFakeNews(statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,speaker,state_info,speaker_job_title): #add your parameters here\n",
        "  accur = [0.2257,0.42] #add your model accuracy here\n",
        "  w = [float(i)/sum(accur) for i in accur]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  if statement!='':\n",
        "    prob.append(w[0] * get_sentiment_score(statement))\n",
        "    sumW += w[0]\n",
        "  \n",
        "  if statement!='':\n",
        "    a,b,c,d,e,f= getStanceScore(statement,barely_true_counts,false_counts,half_true_counts,mostly_true_counts, pants_on_fire_counts,speaker,state_info,speaker_job_title)\n",
        "    A=a*w[0]+b*w[1]/sum(accur)\n",
        "\n",
        "  if statement!='':\n",
        "    combined_models(statement)\n",
        "\n",
        "  ##do similar steps to your model\n",
        " \n",
        "  # probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "  return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTzd_zNsI-Qq",
        "outputId": "2fed6588-43cf-446e-b449-87bf88aa88dd"
      },
      "source": [
        "result = isFakeNews(\"Says his budget provides the highest state funding level in history for education\",28,23,38,34,7,'rick-scott','Florida','Governor')\n",
        "print(\"Result is :\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result is : 0.21978038337884864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD3JJunxI_2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce613b74-e5c8-4c0b-a90d-0d038f709b9e"
      },
      "source": [
        "if result <=0.16:\n",
        "    print('1')\n",
        "elif result >0.16  and result <=0.32:\n",
        "    print('2')\n",
        "elif result >0.32  and result <=0.48:\n",
        "    print('3')\n",
        "elif result >0.48  and result <=0.64:\n",
        "    print('4')\n",
        "elif result >0.64  and result <=0.80:\n",
        "    print('5')\n",
        "elif result >0.80:\n",
        "    print('6')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    }
  ]
}