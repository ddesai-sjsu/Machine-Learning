{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Elite Homework 10 Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddesai-sjsu/Machine-Learning/blob/main/The_Elite_Homework_10_Combined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D6ZgVGPqpCa"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPCcADGlqwwU"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x54qWxy_qz8d"
      },
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OttBdxs1rb4m"
      },
      "source": [
        "## **Sentiment Factor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJzvQQ_DrJ51"
      },
      "source": [
        "### Downloading pickled models of TFIDF Vectorizer & Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KdYIs5kq0II",
        "outputId": "29d7c094-1b4d-43fa-dbfe-ad9774dff9fe"
      },
      "source": [
        "!gdown --id 1Yuxk8aC7PTQJwJz8mw1Xn6uPyEU_vhm6\n",
        "!gdown --id 1gabfL1LLI-iOtc9f5JI5EUyqQuHPHtku\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Yuxk8aC7PTQJwJz8mw1Xn6uPyEU_vhm6\n",
            "To: /content/Neural Net\n",
            "100% 766k/766k [00:00<00:00, 48.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gabfL1LLI-iOtc9f5JI5EUyqQuHPHtku\n",
            "To: /content/tfidf\n",
            "100% 160k/160k [00:00<00:00, 45.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9yNqilBr6c7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfpbnYF8rnxe"
      },
      "source": [
        "def get_POS_tags(statement):\n",
        "  tokens = nltk.word_tokenize(statement)\n",
        "  text = nltk.Text(tokens)\n",
        "  tags = nltk.pos_tag(text)\n",
        "  counts = Counter(tag for word,tag in tags)\n",
        "  noun_counts=counts['NN']+counts['NNS']+counts['NNP']+counts['NNPS']\n",
        "  verb_counts=counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBG']\n",
        "  adverbverb_counts=counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBG']\n",
        "  adjective_counts=counts['JJ']+counts['JJR']+counts['JJS']\n",
        "  possessive_ending_counts=counts['POS']\n",
        "  return noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Wppp6zq7uo"
      },
      "source": [
        "def get_sentiment_score(statement):\n",
        "  model=pickle.load(open('/content/Neural Net', 'rb'))\n",
        "  tfidf_vect=pickle.load(open('/content/tfidf', 'rb'))\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  lem=WordNetLemmatizer()\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "  p_statement= re.sub(r\"\\W\", \" \",str(statement))\n",
        "  p_statement=re.sub(r'[,\\.!?]', \" \",str(p_statement))\n",
        "  p_statement=re.sub(r\" \\d+\", \" \",str(p_statement))\n",
        "  p_statement=word_tokenize(p_statement.lower())\n",
        "  p_statement=' '.join(p_statement)\n",
        "  p_statement=' '.join([word for word in p_statement.split() if word not in stop_words])\n",
        "  p_statement=' '.join([lem.lemmatize(word) for word in p_statement.split()])\n",
        "  p_statement=' '.join([stemmer.stem(word) for word in p_statement.split()])\n",
        "  noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts=get_POS_tags(p_statement)\n",
        "\n",
        "  compound= analyzer.polarity_scores(p_statement)['compound']\n",
        "  neg=  analyzer.polarity_scores(p_statement)['neg'] \n",
        "  neu = analyzer.polarity_scores(p_statement)['neu']\n",
        "  pos = analyzer.polarity_scores(p_statement)['pos']\n",
        "\n",
        "  X=[noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts,compound,neg,neu,pos]\n",
        "  vector=tfidf_vect.transform([p_statement])\n",
        "\n",
        "  final_input=scipy.sparse.hstack((vector,X),format='csr')\n",
        "  \n",
        "  prediction=model.predict(final_input)\n",
        "  prediction_probability=model.predict_proba(final_input)[:,1]\n",
        "  return prediction_probability.item(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6oyLJTq9r7"
      },
      "source": [
        "get_sentiment_score('sad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "wobPpuY7_SIr",
        "outputId": "218c041a-b26e-467f-fd4c-098da52b7a3c"
      },
      "source": [
        "def isFakeNews(statement,): #add your parameters here\n",
        "  accur = [0.2257,] #add your model accuracy here\n",
        "  w = [float(i)/sum(accur) for i in accur]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  if statement!='':\n",
        "    prob.append(w[0] * get_sentiment_score(statement))\n",
        "    sumW += w[0]\n",
        "  ##do similar steps to your model\n",
        " \n",
        "  probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "  return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4a4b3ae83006>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def isFakeNews(statement, ....): #add your parameters here\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-SAeb3YAWHu"
      },
      "source": [
        "# result=isFakeNews(pass parameters here)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebvTRwArAL3w"
      },
      "source": [
        "if result <=0.16:\n",
        "    print('1')\n",
        "elif result >0.16  and result <=0.32:\n",
        "    print('2')\n",
        "elif result >0.32  and result <=0.48:\n",
        "    print('3')\n",
        "elif result >0.48  and result <=0.64:\n",
        "    print('4')\n",
        "elif result >0.64  and result <=0.80:\n",
        "    print('5')\n",
        "elif result >0.80:\n",
        "    print('6')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}