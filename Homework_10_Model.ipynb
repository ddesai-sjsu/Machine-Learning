{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 10 Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9fDWIhesbFwmL0HtD4Jl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddesai-sjsu/Machine-Learning/blob/main/Homework_10_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D6ZgVGPqpCa"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPCcADGlqwwU"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x54qWxy_qz8d"
      },
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F8uTtUsq0gD"
      },
      "source": [
        "def get_POS_tags(statement):\n",
        "  tokens = nltk.word_tokenize(statement)\n",
        "  text = nltk.Text(tokens)\n",
        "  tags = nltk.pos_tag(text)\n",
        "  counts = Counter(tag for word,tag in tags)\n",
        "  noun_counts=counts['NN']+counts['NNS']+counts['NNP']+counts['NNPS']\n",
        "  verb_counts=counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBG']\n",
        "  adverbverb_counts=counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBG']\n",
        "  adjective_counts=counts['JJ']+counts['JJR']+counts['JJS']\n",
        "  possessive_ending_counts=counts['POS']\n",
        "  return noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Wppp6zq7uo"
      },
      "source": [
        "def get_sentiment_score(statement):\n",
        "  model=pickle.load(open('/content/Neural Net', 'rb'))\n",
        "  tfidf_vect=pickle.load(open('/content/tfidf', 'rb'))\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  lem=WordNetLemmatizer()\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "  p_statement= re.sub(r\"\\W\", \" \",str(statement))\n",
        "  p_statement=re.sub(r'[,\\.!?]', \" \",str(p_statement))\n",
        "  p_statement=re.sub(r\" \\d+\", \" \",str(p_statement))\n",
        "  p_statement=word_tokenize(p_statement.lower())\n",
        "  p_statement=' '.join(p_statement)\n",
        "  p_statement=' '.join([word for word in p_statement.split() if word not in stop_words])\n",
        "  p_statement=' '.join([lem.lemmatize(word) for word in p_statement.split()])\n",
        "  p_statement=' '.join([stemmer.stem(word) for word in p_statement.split()])\n",
        "  noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts=get_POS_tags(p_statement)\n",
        "\n",
        "  compound= analyzer.polarity_scores(p_statement)['compound']\n",
        "  neg=  analyzer.polarity_scores(p_statement)['neg'] \n",
        "  neu = analyzer.polarity_scores(p_statement)['neu']\n",
        "  pos = analyzer.polarity_scores(p_statement)['pos']\n",
        "\n",
        "  X=[noun_counts,verb_counts,adverbverb_counts,adjective_counts,possessive_ending_counts,compound,neg,neu,pos]\n",
        "  vector=tfidf_vect.transform([p_statement])\n",
        "\n",
        "  final_input=scipy.sparse.hstack((vector,X),format='csr')\n",
        "  \n",
        "  prediction=model.predict(final_input)\n",
        "  prediction_probability=model.predict_proba(final_input)[:,1]\n",
        "  return prediction_probability.item(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6oyLJTq9r7"
      },
      "source": [
        "get_sentiment_score('sad')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}